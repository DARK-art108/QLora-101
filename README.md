### **QLora-101 : Quantized LLMs with Low-Rank Adapters, Fine Tuning GPT-NeoX-12B**

LoRa adds a tiny amount of trainable parameters, i.e., adapters, for each layer of the LLM and freezes all the original parameters. For fine-tuning, we only have to update the adapter weights which significantly reduces the memory footprint.

![alt text](https://miro.medium.com/v2/resize:fit:2000/1*PqZCb0Ll1i-ibx3Ht2VbGQ.png)
